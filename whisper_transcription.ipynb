{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaushalapptware/colab/blob/main/whisper_transcription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=64px>Whisper Transcription</font>\n",
        "\n",
        "Notebook built by Trelis Research. Find us at [Trelis.com](https://trelis.com), on [YouTube](https://YouTube.com/@TrelisResearch) and on [HuggingFace](https://huggingface.co/Trelis).\n",
        "\n",
        "*Trelis Research emails members each time a new video tutorial is published. If you'd like, you can join [here](https://trelis.substack.com).*\n",
        "\n",
        "Built upon an [original notebook](https://colab.research.google.com/github/deepgram-devs/try-whisper-in-google-collab/blob/main/try_whisper_in_three_easy_steps.ipynb) by Ross O'Connell."
      ],
      "metadata": {
        "id": "9qzwD9ts4_kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive (optional)\n",
        "You also need to change the paths below for pulling and transcribing audio."
      ],
      "metadata": {
        "id": "biPhaxG7LHPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_KKivchsPXCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "cache_dir = \"/content/drive/My Drive/video_transcripts\"\n",
        "os.makedirs(cache_dir, exist_ok=True) # Ensure the directory exists"
      ],
      "metadata": {
        "id": "XdeaEucBPYim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Whisper"
      ],
      "metadata": {
        "id": "tATxgEBmLK_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first line we install Whisper! (see [HuggingFace](https://huggingface.co/openai/whisper-small) for more details)"
      ],
      "metadata": {
        "id": "V1QHcVQz4Gu7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOvKw2K3kWqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd62e595-b30e-4f56-9832-0726e49ad134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grab and transcribe some audio from YouTube"
      ],
      "metadata": {
        "id": "FatSKi3YAQCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp -q -U"
      ],
      "metadata": {
        "id": "0NqzXPOXQ6JQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de2e384-aad0-4c5e-993b-8bb1eaf4c1b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yt-dlp https://youtu.be/zmf1Kujygt8 --format m4a -o \"/content/%(id)s.%(ext)s\"\n",
        "!whisper \"/content/zmf1Kujygt8.m4a\" --model small --language English"
      ],
      "metadata": {
        "id": "cYaGfY1J2VRi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461f8dc5-e523-4738-c285-4b4c5fbbf865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://youtu.be/zmf1Kujygt8\n",
            "[youtube] zmf1Kujygt8: Downloading webpage\n",
            "[youtube] zmf1Kujygt8: Downloading ios player API JSON\n",
            "[youtube] zmf1Kujygt8: Downloading android player API JSON\n",
            "[youtube] zmf1Kujygt8: Downloading m3u8 information\n",
            "[info] zmf1Kujygt8: Downloading 1 format(s): 140\n",
            "[download] Destination: /content/zmf1Kujygt8.m4a\n",
            "\u001b[K[download] 100% of   51.60MiB in \u001b[1;37m00:00:25\u001b[0m at \u001b[0;32m1.99MiB/s\u001b[0m\n",
            "[FixupM4a] Correcting container of \"/content/zmf1Kujygt8.m4a\"\n",
            "[00:00.000 --> 00:05.000]  I'm going to walk you through data extraction using large language models.\n",
            "[00:05.000 --> 00:11.000]  We're going to extract names and organizations from short and really long pieces of text.\n",
            "[00:11.000 --> 00:16.000]  We'll take a look at extracting into JSON objects and also in YAML format.\n",
            "[00:16.000 --> 00:21.000]  I'll talk about the cost of different approaches, GPT 3.5, GPT 4,\n",
            "[00:21.000 --> 00:28.000]  and then the best open source model in my opinion for extraction, OpenChat 3.5.7b.\n",
            "[00:28.000 --> 00:31.000]  So let's take a look at the agenda.\n",
            "[00:31.000 --> 00:34.000]  We'll talk a little bit about motivation.\n",
            "[00:34.000 --> 00:37.000]  Then I'll show you the two different formats we're going to use for extraction.\n",
            "[00:37.000 --> 00:42.000]  I'll go through some of the key problems and why extracting data is difficult.\n",
            "[00:42.000 --> 00:47.000]  Then I'll talk about some of the best models, the best open source model,\n",
            "[00:47.000 --> 00:52.000]  and then the best private model I've tried is GPT 4 unsurprisingly.\n",
            "[00:52.000 --> 00:59.000]  I'll go through a demo then where I compared a performance using OpenChat 3.5 versus GPT 3.5 and GPT 4.\n",
            "[00:59.000 --> 01:02.000]  I'll show you short examples, which are more illustrative,\n",
            "[01:02.000 --> 01:09.000]  and then I'll show you long examples and a technique where you can concurrently make requests to a server I'll set up,\n",
            "[01:09.000 --> 01:14.000]  and that gets a very high throughput, so you can do extraction very fast in parallel\n",
            "[01:14.000 --> 01:20.000]  by pinging the server with different chunks of the text that you want to extract data from.\n",
            "[01:20.000 --> 01:27.000]  I'll do, if I have time, a little demo on Notux, which is a fine-tune of the mixed-rall model.\n",
            "[01:27.000 --> 01:33.000]  So OpenChat is actually a fine-tune of the mixed-rall 7b,\n",
            "[01:33.000 --> 01:38.000]  and Notux is a fine-tune of the mixed-rall, so some kind of similarity there.\n",
            "[01:38.000 --> 01:41.000]  Then I want to talk a little bit about costing.\n",
            "[01:41.000 --> 01:44.000]  When you're using GPT 4, it is quite expensive,\n",
            "[01:44.000 --> 01:49.000]  so I'll go through the costs when you're extracting per 100,000 tokens of text,\n",
            "[01:49.000 --> 01:54.000]  and I'll do that for GPT 4, GPT 3.5, and then for OpenChat.\n",
            "[01:54.000 --> 01:58.000]  In terms of motivation for data extraction, it's quite simple.\n",
            "[01:58.000 --> 02:02.000]  If you have a very long piece of text and you don't use a language model,\n",
            "[02:02.000 --> 02:06.000]  it's going to be very hard to write a program that will just extract names.\n",
            "[02:06.000 --> 02:09.000]  You would have to do some kind of pattern-matching,\n",
            "[02:09.000 --> 02:14.000]  maybe find common names like Michael or Michelle or something,\n",
            "[02:14.000 --> 02:17.000]  and see if they occur by direct text comparison.\n",
            "[02:17.000 --> 02:20.000]  You could also do some kind of vector search.\n",
            "[02:20.000 --> 02:25.000]  Vector search is where you would embed, basically convert your text into vectors,\n",
            "[02:25.000 --> 02:29.000]  and then you would define some terms that you're looking for\n",
            "[02:29.000 --> 02:31.000]  and get the vectors for those terms,\n",
            "[02:31.000 --> 02:35.000]  and by comparing the set of vectors in your text versus the terms you're looking for,\n",
            "[02:35.000 --> 02:41.000]  you can determine whether the search vectors appear within your database.\n",
            "[02:41.000 --> 02:45.000]  Now, that's generally not perfectly exact, so also difficult to pull off.\n",
            "[02:45.000 --> 02:48.000]  And this is really the beauty of using a language model.\n",
            "[02:48.000 --> 02:51.000]  It's extremely flexible, whereby you can give it a prompt to ask\n",
            "[02:51.000 --> 02:54.000]  for the names to be returned in a certain format,\n",
            "[02:54.000 --> 02:58.000]  and then it's able to use much more meaning, much more precisely,\n",
            "[02:58.000 --> 03:01.000]  to get you the names or the organizations.\n",
            "[03:01.000 --> 03:04.000]  Now, I will show just names and organizations today.\n",
            "[03:04.000 --> 03:06.000]  I'll show examples where I'm extracting both,\n",
            "[03:06.000 --> 03:09.000]  which is definitely more difficult and less robust\n",
            "[03:09.000 --> 03:11.000]  than trying to extract one type of data,\n",
            "[03:11.000 --> 03:13.000]  but I wanted to challenge the models a little.\n",
            "[03:13.000 --> 03:16.000]  You could, of course, tweak the examples here\n",
            "[03:16.000 --> 03:20.000]  if you want to extract numbers, maybe stock prices, number of objects,\n",
            "[03:20.000 --> 03:22.000]  different data formats.\n",
            "[03:22.000 --> 03:25.000]  That would all be possible in an analogous way.\n",
            "[03:25.000 --> 03:30.000]  Now, some of the problems that you run into trying to extract data,\n",
            "[03:30.000 --> 03:33.000]  I've put them into three rough buckets here.\n",
            "[03:33.000 --> 03:37.000]  And actually, first, before I talk about the problems,\n",
            "[03:37.000 --> 03:41.000]  I think it's probably helpful to show the YAML and the JSON structures.\n",
            "[03:41.000 --> 03:43.000]  I'll just jump right into an example here.\n",
            "[03:43.000 --> 03:47.000]  So let me first show you an example of JSON extraction.\n",
            "[03:47.000 --> 03:50.000]  So this is one data format that can be used.\n",
            "[03:50.000 --> 03:54.000]  And here you see a JSON object.\n",
            "[03:54.000 --> 03:57.000]  So I've asked from this long transcript,\n",
            "[03:57.000 --> 03:59.000]  which is the Berkshire Hathaway meeting from last year,\n",
            "[03:59.000 --> 04:03.000]  I've asked GPT-4 to extract a list of names and organizations\n",
            "[04:03.000 --> 04:06.000]  and return it in a JSON object.\n",
            "[04:06.000 --> 04:10.000]  And you can just see here, I just want you to take in the rough format.\n",
            "[04:10.000 --> 04:12.000]  This is perfectly formatted.\n",
            "[04:12.000 --> 04:14.000]  And because it has this predictable format,\n",
            "[04:14.000 --> 04:17.000]  we can easily write a Python or other script\n",
            "[04:17.000 --> 04:20.000]  and manage and control this data as we wish from here.\n",
            "[04:20.000 --> 04:23.000]  So this is what the JSON format looks like.\n",
            "[04:23.000 --> 04:26.000]  I'll also show you this YAML format.\n",
            "[04:26.000 --> 04:30.000]  So YAML is actually a little bit simpler and cleaner.\n",
            "[04:30.000 --> 04:33.000]  You've got the heading here, names, a colon,\n",
            "[04:33.000 --> 04:36.000]  and then just a dash for each of the entries,\n",
            "[04:36.000 --> 04:38.000]  and then the same for organizations.\n",
            "[04:38.000 --> 04:40.000]  So comparing YAML and JSON,\n",
            "[04:40.000 --> 04:43.000]  you probably use yet less tokens up.\n",
            "[04:43.000 --> 04:46.000]  If you really want to minimize costs,\n",
            "[04:46.000 --> 04:50.000]  you probably use less tokens by doing a YAML type approach.\n",
            "[04:50.000 --> 04:54.000]  Now, as you'll see when I go through some live examples,\n",
            "[04:54.000 --> 04:57.000]  in some cases the YAML performs a little better.\n",
            "[04:57.000 --> 04:59.000]  In some cases the JSON performs better\n",
            "[04:59.000 --> 05:02.000]  and it will depend on how the model has been trained.\n",
            "[05:02.000 --> 05:04.000]  I'm not saying I understand the cases\n",
            "[05:04.000 --> 05:07.000]  of exactly where one performs well and the other doesn't.\n",
            "[05:07.000 --> 05:10.000]  But hopefully I can give you a somewhat nuanced understanding\n",
            "[05:10.000 --> 05:13.000]  just by showing you some live examples.\n",
            "[05:13.000 --> 05:16.000]  So what we're going to do in the next step\n",
            "[05:16.000 --> 05:18.000]  is we're going to generate some prompts,\n",
            "[05:18.000 --> 05:20.000]  and I'll talk about that.\n",
            "[05:20.000 --> 05:22.000]  Maybe it's helpful if I show you\n",
            "[05:22.000 --> 05:24.000]  some of the exact prompts that I've used.\n",
            "[05:24.000 --> 05:28.000]  So let's scroll right up to the top here.\n",
            "[05:28.000 --> 05:32.000]  So here let's go through the YAML format.\n",
            "[05:32.000 --> 05:34.000]  So the prompt is as follows.\n",
            "[05:34.000 --> 05:37.000]  Extract names and organizations from the provided text\n",
            "[05:37.000 --> 05:39.000]  and return them in YAML format.\n",
            "[05:39.000 --> 05:42.000]  Use the following schema.\n",
            "[05:42.000 --> 05:45.000]  So I then give an example of the schema that I want used.\n",
            "[05:45.000 --> 05:48.000]  So I have properties and names,\n",
            "[05:48.000 --> 05:52.000]  and the names we've got string and array of strings,\n",
            "[05:52.000 --> 05:55.000]  and the organizations are also going to be an array of strings,\n",
            "[05:55.000 --> 05:57.000]  and we're going to require that the language model return\n",
            "[05:57.000 --> 06:01.000]  names and organizations.\n",
            "[06:01.000 --> 06:03.000]  Then continuing on with the prompt,\n",
            "[06:03.000 --> 06:06.000]  I say here is an example of response in YAML format.\n",
            "[06:06.000 --> 06:09.000]  So not only do I describe what the schema is,\n",
            "[06:09.000 --> 06:12.000]  I also give an example of a response.\n",
            "[06:12.000 --> 06:14.000]  So names and then sample strings,\n",
            "[06:14.000 --> 06:16.000]  and then organization and sample strings.\n",
            "[06:16.000 --> 06:19.000]  And this is exactly the format that I want the model to respond in.\n",
            "[06:19.000 --> 06:21.000]  Putting in that one example here,\n",
            "[06:21.000 --> 06:24.000]  that makes it a one-shot request to the language model.\n",
            "[06:24.000 --> 06:27.000]  This definitely helps with performance.\n",
            "[06:27.000 --> 06:29.000]  Now I give a little extra guidance,\n",
            "[06:29.000 --> 06:32.000]  so I say do not include anything that's not explicitly mentioned in the text.\n",
            "[06:32.000 --> 06:35.000]  Analyze the text carefully to ensure all requested data is extracted.\n",
            "[06:35.000 --> 06:38.000]  Include each name and organization only once,\n",
            "[06:38.000 --> 06:41.000]  avoiding repetition in the list of organizations,\n",
            "[06:41.000 --> 06:43.000]  which is a problem you'll see.\n",
            "[06:43.000 --> 06:47.000]  Adhere strictly to the response format without adding extra spaces or text.\n",
            "[06:47.000 --> 06:50.000]  So what you want is the model to respond just with the object.\n",
            "[06:50.000 --> 06:52.000]  You don't want it to say,\n",
            "[06:52.000 --> 06:54.000]  oh, here's the object, you're very welcome.\n",
            "[06:54.000 --> 06:57.000]  Or give the object and say, do you have any other questions?\n",
            "[06:57.000 --> 06:59.000]  You want it to be very predictable\n",
            "[06:59.000 --> 07:02.000]  what the model is going to respond with.\n",
            "[07:02.000 --> 07:05.000]  Now it's true that if there's something before or after,\n",
            "[07:05.000 --> 07:07.000]  you could just slice that off programmatically,\n",
            "[07:07.000 --> 07:11.000]  but still it adds an area where you get unpredictability,\n",
            "[07:11.000 --> 07:15.000]  and that's not what you want when you're trying to do extraction.\n",
            "[07:15.000 --> 07:17.000]  So after this pre-prompt here,\n",
            "[07:17.000 --> 07:20.000]  what comes next is the actual text.\n",
            "[07:20.000 --> 07:23.000]  And if you're going over the context length,\n",
            "[07:23.000 --> 07:26.000]  then this would not just be all of the text,\n",
            "[07:26.000 --> 07:28.000]  it would be a chunk of the text.\n",
            "[07:28.000 --> 07:32.000]  And so by repeating this entire prompt multiple times in parallel,\n",
            "[07:32.000 --> 07:34.000]  you can treat each chunk separately,\n",
            "[07:34.000 --> 07:39.000]  and then you can accumulate your results across those chunks at the end.\n",
            "[07:39.000 --> 07:43.000]  So anyway, after the pre-prompt, we start off the text,\n",
            "[07:43.000 --> 07:47.000]  and we just inject the text right here.\n",
            "[07:47.000 --> 07:50.000]  Now this is a really long piece of text.\n",
            "[07:50.000 --> 07:52.000]  When we get to the end of the text,\n",
            "[07:52.000 --> 07:56.000]  we close things off by saying text end.\n",
            "[07:57.000 --> 08:02.000]  And this just gives a little delimiter to tell the model that,\n",
            "[08:02.000 --> 08:04.000]  okay, here is the chunk of text,\n",
            "[08:04.000 --> 08:06.000]  here's where it starts, here's where it ends,\n",
            "[08:06.000 --> 08:09.000]  and that adds a little bit of clarity to the entire process.\n",
            "[08:09.000 --> 08:13.000]  Now what I like to do then is actually give a final prompt\n",
            "[08:13.000 --> 08:15.000]  after having injected the text.\n",
            "[08:15.000 --> 08:18.000]  So you can give all the prompt before the text,\n",
            "[08:18.000 --> 08:21.000]  but just by putting that very last piece of text\n",
            "[08:21.000 --> 08:25.000]  and knowing that models pay attention particularly to the very start\n",
            "[08:25.000 --> 08:27.000]  and the end of the prompts,\n",
            "[08:27.000 --> 08:29.000]  it's nice to put this little piece at the end,\n",
            "[08:29.000 --> 08:31.000]  it can improve performance as well,\n",
            "[08:31.000 --> 08:34.000]  and just kind of reiterate what the model is to do.\n",
            "[08:34.000 --> 08:37.000]  So I say now or answer immediately and only.\n",
            "[08:37.000 --> 08:41.000]  So again, encouraging the model just to provide the object of the response\n",
            "[08:41.000 --> 08:43.000]  in YAML format.\n",
            "[08:43.000 --> 08:49.000]  And with that, we have the YAML format being provided by GPT-4.\n",
            "[08:49.000 --> 08:53.000]  Now let me just go through a little more quickly the JSON example.\n",
            "[08:53.000 --> 08:57.000]  Again, I've actually considered the exact same piece of text,\n",
            "[08:57.000 --> 08:59.000]  and you'll see the prompt is very similar.\n",
            "[08:59.000 --> 09:02.000]  Extract names and organizations are turned them in JSON format.\n",
            "[09:02.000 --> 09:04.000]  Use the following schema,\n",
            "[09:04.000 --> 09:08.000]  but now you see the schema is a JSON type schema,\n",
            "[09:08.000 --> 09:13.000]  and the example I provide is also a JSON type example.\n",
            "[09:13.000 --> 09:18.000]  So then I say do not include anything that's not explicitly mentioned in the text.\n",
            "[09:18.000 --> 09:21.000]  So here I'm just reiterating the same guidance,\n",
            "[09:21.000 --> 09:24.000]  trying to encourage the model not to miss anything\n",
            "[09:24.000 --> 09:27.000]  by saying all the requested data is extracted,\n",
            "[09:27.000 --> 09:30.000]  trying to avoid repetition here by saying not to repeat anything,\n",
            "[09:30.000 --> 09:34.000]  and then trying to ensure nothing comes before or after the response.\n",
            "[09:34.000 --> 09:39.000]  And again, I put in the entire text wrapped in text start and text end\n",
            "[09:39.000 --> 09:41.000]  just to provide some clear delimiters,\n",
            "[09:41.000 --> 09:46.000]  and I finish off with the one sentence prompt at the end which says,\n",
            "[09:46.000 --> 09:49.000]  now answer immediately and only in JSON format,\n",
            "[09:49.000 --> 09:55.000]  which GPT-4 does obligingly.\n",
            "[09:55.000 --> 09:58.000]  So there's one other thing I want to show you here\n",
            "[09:58.000 --> 10:00.000]  that when I run these models,\n",
            "[10:00.000 --> 10:04.000]  you can see I'm running with a GPT-4 turbo model right here.\n",
            "[10:04.000 --> 10:09.000]  I also have got examples where I run with GPT-3.5 turbo,\n",
            "[10:09.000 --> 10:11.000]  and you'll also see the temperature.\n",
            "[10:11.000 --> 10:14.000]  It's quite small on my screen, but I've set the temperature to zero.\n",
            "[10:14.000 --> 10:16.000]  I would definitely recommend setting temperature to zero.\n",
            "[10:16.000 --> 10:19.000]  You definitely want the highest probability tokens.\n",
            "[10:19.000 --> 10:23.000]  You don't want some kind of variance that will take you away from repetition.\n",
            "[10:23.000 --> 10:26.000]  You want to have probably low temperature in this case.\n",
            "[10:26.000 --> 10:29.000]  Now, if you're running with chat GPT,\n",
            "[10:29.000 --> 10:31.000]  that can be more difficult,\n",
            "[10:31.000 --> 10:34.000]  which is why I'm using chat.trails.com.\n",
            "[10:34.000 --> 10:36.000]  This is just a fork of chatbot UI.\n",
            "[10:36.000 --> 10:39.000]  I believe chatbot UI are coming out with a new version\n",
            "[10:39.000 --> 10:43.000]  that will support GPT-4 API.\n",
            "[10:43.000 --> 10:47.000]  But what this allows you to do is put in an open AI API key,\n",
            "[10:47.000 --> 10:49.000]  which means, sadly, that you will be paying per token.\n",
            "[10:49.000 --> 10:53.000]  But when you create a new chat, you have the flexibility here.\n",
            "[10:53.000 --> 10:56.000]  And this is open source and available.\n",
            "[10:56.000 --> 10:58.000]  You could either run your own instance\n",
            "[10:58.000 --> 11:01.000]  or use chatbottrails.com input your API key.\n",
            "[11:01.000 --> 11:03.000]  I don't have any access to your data.\n",
            "[11:03.000 --> 11:07.000]  All of these conversations are stored locally on your browser.\n",
            "[11:07.000 --> 11:10.000]  Now, you can see the beauty is that I can select\n",
            "[11:10.000 --> 11:12.000]  these longer context models\n",
            "[11:12.000 --> 11:14.000]  that are possible with even a plus subscription.\n",
            "[11:14.000 --> 11:18.000]  The plus subscription allows for 32K tokens as of now.\n",
            "[11:18.000 --> 11:20.000]  This goes up to 128.\n",
            "[11:20.000 --> 11:24.000]  And further, you can use the GPT-3.5 Turbo\n",
            "[11:24.000 --> 11:26.000]  and even the instruct model if you wanted.\n",
            "[11:26.000 --> 11:30.000]  But also, you can lower this temperature\n",
            "[11:30.000 --> 11:33.000]  so that you get a more focused and deterministic,\n",
            "[11:33.000 --> 11:34.000]  or in other words,\n",
            "[11:34.000 --> 11:38.000]  get the model to pick the most likely token at each stage.\n",
            "[11:38.000 --> 11:41.000]  So with that, I've gone through the main formats\n",
            "[11:41.000 --> 11:44.000]  and I've shown you the examples in GPT-4 and 3.5.\n",
            "[11:44.000 --> 11:47.000]  I haven't commented yet on the accuracy,\n",
            "[11:47.000 --> 11:50.000]  which is hard to say without either reviewing manually\n",
            "[11:50.000 --> 11:53.000]  as a human or comparing to some other model.\n",
            "[11:53.000 --> 11:55.000]  So we'll get to the comparison later.\n",
            "[11:55.000 --> 11:57.000]  But first, I just want to highlight a few problems,\n",
            "[11:57.000 --> 12:00.000]  many of which are addressed by the prompt that I showed,\n",
            "[12:00.000 --> 12:04.000]  but there are some other factors that we'll come upon too.\n",
            "[12:04.000 --> 12:07.000]  So some of the main challenges with extracting data\n",
            "[12:07.000 --> 12:11.000]  are first that you get incorrect syntax.\n",
            "[12:11.000 --> 12:13.000]  So I showed you some prompts that worked,\n",
            "[12:13.000 --> 12:16.000]  but occasionally and definitely depending on the model,\n",
            "[12:16.000 --> 12:19.000]  it will just respond with a slightly different syntax\n",
            "[12:19.000 --> 12:21.000]  that's not the correct JSON\n",
            "[12:21.000 --> 12:23.000]  or it's not the exact syntax you wanted.\n",
            "[12:23.000 --> 12:25.000]  And that will make it very difficult to parse\n",
            "[12:25.000 --> 12:29.000]  because it's unpredictable exactly what syntax will be provided.\n",
            "[12:29.000 --> 12:33.000]  Another problem is, and I see this a lot with the mixed trial model,\n",
            "[12:33.000 --> 12:36.000]  which I won't be running because I'll run the fine-tune notux,\n",
            "[12:36.000 --> 12:37.000]  which is a little better,\n",
            "[12:37.000 --> 12:41.000]  but mixed trial tends to add a lot of context before and afterwards,\n",
            "[12:41.000 --> 12:44.000]  and that makes it difficult to do the parsing.\n",
            "[12:44.000 --> 12:47.000]  And then a third key issue is repetition.\n",
            "[12:47.000 --> 12:50.000]  You ask the model to extract, say, the names,\n",
            "[12:50.000 --> 12:52.000]  and it just keeps saying something like\n",
            "[12:52.000 --> 12:54.000]  Warren Buffett, Warren Buffett, repeatedly.\n",
            "[12:54.000 --> 12:57.000]  And this is a common issue for language models.\n",
            "[12:57.000 --> 13:00.000]  They get stuck in a pattern where they're just reproducing\n",
            "[13:00.000 --> 13:03.000]  the same output tokens in the same pattern,\n",
            "[13:03.000 --> 13:06.000]  and this results in using up the full maximum tokens\n",
            "[13:06.000 --> 13:08.000]  you've specified for the output,\n",
            "[13:08.000 --> 13:10.000]  so you'll get slow responses\n",
            "[13:10.000 --> 13:14.000]  and also just a needless amount of the same name being returned.\n",
            "[13:14.000 --> 13:17.000]  Moreover, once you get stuck on a given name,\n",
            "[13:17.000 --> 13:20.000]  it's not going to find any other names that are in the passage,\n",
            "[13:20.000 --> 13:23.000]  so it means it's going to fail to extract all of the names\n",
            "[13:23.000 --> 13:25.000]  or the organizations.\n",
            "[13:25.000 --> 13:28.000]  So these are the key issues that we see\n",
            "[13:28.000 --> 13:30.000]  and what we're going to try to address\n",
            "[13:30.000 --> 13:33.000]  when we go through some of the live demo.\n",
            "[13:33.000 --> 13:36.000]  So very briefly, before I start the demo on OpenChat,\n",
            "[13:36.000 --> 13:39.000]  I want to talk about a few models I've looked at,\n",
            "[13:39.000 --> 13:42.000]  and there are many that I've tried.\n",
            "[13:42.000 --> 13:45.000]  Unfortunately, many have not worked as well as I'd hoped.\n",
            "[13:45.000 --> 13:47.000]  But maybe somewhat surprisingly,\n",
            "[13:47.000 --> 13:50.000]  one of the best models I've found is a 7B model.\n",
            "[13:50.000 --> 13:53.000]  It's the OpenChat model 3.5,\n",
            "[13:53.000 --> 13:55.000]  and you'll see as in the testing,\n",
            "[13:55.000 --> 13:58.000]  it's comparable to a GBT 3.5 turbo.\n",
            "[13:58.000 --> 14:00.000]  It's, by the way, somewhat coincidental\n",
            "[14:00.000 --> 14:02.000]  that it has the same naming here,\n",
            "[14:02.000 --> 14:04.000]  and that's a bit misleading.\n",
            "[14:04.000 --> 14:08.000]  OpenChat is a fine tune of the Mistrials 7B model.\n",
            "[14:08.000 --> 14:11.000]  In some cases, it's almost as good as GBT4,\n",
            "[14:11.000 --> 14:13.000]  and maybe for real long passages,\n",
            "[14:13.000 --> 14:16.000]  it's able to generate a much lower cost of extraction\n",
            "[14:16.000 --> 14:20.000]  than GBT4 and somewhat similar quality.\n",
            "[14:20.000 --> 14:23.000]  Now, on a technical note, OpenChat 3.5\n",
            "[14:23.000 --> 14:25.000]  had released two more updated models,\n",
            "[14:25.000 --> 14:27.000]  one in December and one in January,\n",
            "[14:27.000 --> 14:29.000]  so there are three models available in Hugging Face Hub.\n",
            "[14:29.000 --> 14:31.000]  We're going to use the original one.\n",
            "[14:31.000 --> 14:34.000]  For some reason, I find that the more recent models,\n",
            "[14:34.000 --> 14:37.000]  they perform a little worse on function calling,\n",
            "[14:37.000 --> 14:40.000]  which is an area I'm familiar with from other videos,\n",
            "[14:40.000 --> 14:42.000]  but also they tend to hallucinate a little bit more\n",
            "[14:42.000 --> 14:44.000]  for reasons I don't understand.\n",
            "[14:44.000 --> 14:48.000]  So we're going to stick to the base OpenChat 3.5 model.\n",
            "[14:48.000 --> 14:52.000]  Another model that works quite well is Codelama 34B.\n",
            "[14:52.000 --> 14:54.000]  Now, because it's much bigger,\n",
            "[14:54.000 --> 14:56.000]  so it's about five times bigger than the 7B model,\n",
            "[14:56.000 --> 14:58.000]  it's quite a lot slower to inference,\n",
            "[14:58.000 --> 15:01.000]  so that will make it more expensive as a solution.\n",
            "[15:01.000 --> 15:05.000]  It tends to provide the correct syntax,\n",
            "[15:05.000 --> 15:09.000]  but it often misses entries within the input.\n",
            "[15:09.000 --> 15:14.000]  So in many cases, I still think the 7B model is somewhat better.\n",
            "[15:14.000 --> 15:16.000]  Now, when I chose models to look at,\n",
            "[15:16.000 --> 15:18.000]  I gravitated towards coding models.\n",
            "[15:18.000 --> 15:21.000]  They often are very good with recognizing positions.\n",
            "[15:21.000 --> 15:23.000]  They're good at retrieval tasks,\n",
            "[15:23.000 --> 15:26.000]  I think because code helps them a lot on positioning\n",
            "[15:26.000 --> 15:28.000]  and structured responses.\n",
            "[15:28.000 --> 15:31.000]  So I'm always tempted to try out the DeepSeq Coder models,\n",
            "[15:31.000 --> 15:33.000]  which perform very well.\n",
            "[15:33.000 --> 15:35.000]  Unfortunately, the drawback with the DeepSeq Coder models,\n",
            "[15:35.000 --> 15:37.000]  at least the Instruct models,\n",
            "[15:37.000 --> 15:39.000]  is that they are instruction fine-tuned\n",
            "[15:39.000 --> 15:43.000]  and guided towards only answering if you have a code question.\n",
            "[15:43.000 --> 15:46.000]  And that's true even if you leave out the system prompt\n",
            "[15:46.000 --> 15:49.000]  that tells the model only to respond to code questions.\n",
            "[15:49.000 --> 15:51.000]  You could use the base model,\n",
            "[15:51.000 --> 15:54.000]  it becomes less predictable, somewhat like mixed-dral,\n",
            "[15:54.000 --> 15:57.000]  and doesn't necessarily complete the sentences.\n",
            "[15:57.000 --> 15:59.000]  It will tend to blab on with answers\n",
            "[15:59.000 --> 16:02.000]  even when you want it to stop.\n",
            "[16:02.000 --> 16:05.000]  As I said now a few times, mixed-dral,\n",
            "[16:05.000 --> 16:07.000]  the syntax is typically wrong,\n",
            "[16:07.000 --> 16:10.000]  and that means it's not a good model for this application.\n",
            "[16:10.000 --> 16:13.000]  Noteux 8.7B, as we'll show at the end,\n",
            "[16:13.000 --> 16:15.000]  it generates decent syntax.\n",
            "[16:15.000 --> 16:17.000]  It's a bit inconsistent on accuracy,\n",
            "[16:17.000 --> 16:20.000]  by which I mean it often leaves out entries.\n",
            "[16:20.000 --> 16:22.000]  And the prompt, it's very sensitive to the prompt,\n",
            "[16:22.000 --> 16:24.000]  so small changes in the prompt\n",
            "[16:24.000 --> 16:27.000]  tend to make big changes in the output.\n",
            "[16:27.000 --> 16:30.000]  And you'll see this in general with weaker models.\n",
            "[16:30.000 --> 16:33.000]  The beauty of GPT-4 is that it's quite robust.\n",
            "[16:33.000 --> 16:35.000]  Even if your prompt is not quite right,\n",
            "[16:35.000 --> 16:38.000]  it might have typos, or it might not be optimally structured.\n",
            "[16:38.000 --> 16:40.000]  GPT-4 in most cases,\n",
            "[16:40.000 --> 16:42.000]  and we'll see some cases where it doesn't,\n",
            "[16:42.000 --> 16:44.000]  but in most cases it's very robust to that.\n",
            "[16:44.000 --> 16:47.000]  And OpenChat 3.5 is quite robust as well,\n",
            "[16:47.000 --> 16:50.000]  but even on some of the bigger stretch cases,\n",
            "[16:50.000 --> 16:52.000]  you'll see that small changes in prompts\n",
            "[16:52.000 --> 16:54.000]  can make a difference.\n",
            "[16:54.000 --> 16:56.000]  For the performance comparison,\n",
            "[16:56.000 --> 16:59.000]  I'm going to be using the advanced inference scripts.\n",
            "[16:59.000 --> 17:01.000]  You can purchase access,\n",
            "[17:01.000 --> 17:03.000]  lifetime access to this repository,\n",
            "[17:03.000 --> 17:06.000]  which now contains a lot of scripts\n",
            "[17:06.000 --> 17:09.000]  for a server setup on EC2,\n",
            "[17:09.000 --> 17:13.000]  RunPod or Vast.ai, API setup guides for VLLM and TGI,\n",
            "[17:13.000 --> 17:15.000]  and now a number of scripts,\n",
            "[17:15.000 --> 17:17.000]  including function calling, speed tests,\n",
            "[17:17.000 --> 17:20.000]  and just added today, data extraction scripts.\n",
            "[17:20.000 --> 17:23.000]  Now, if you just want the data extraction scripts,\n",
            "[17:23.000 --> 17:25.000]  you can purchase those on an individual basis here,\n",
            "[17:25.000 --> 17:29.000]  rather than purchasing lifetime access to the repo,\n",
            "[17:29.000 --> 17:31.000]  which includes further updates.\n",
            "[17:31.000 --> 17:33.000]  Now, if you don't want to purchase at all,\n",
            "[17:33.000 --> 17:36.000]  you can make use, of course, of the prompts,\n",
            "[17:36.000 --> 17:38.000]  and hopefully I've given enough detail\n",
            "[17:38.000 --> 17:40.000]  in the earlier part of the video\n",
            "[17:40.000 --> 17:42.000]  to help you just copy those prompts\n",
            "[17:42.000 --> 17:45.000]  and put them in to a piece of code yourself.\n",
            "[17:46.000 --> 17:49.000]  So, here I am in the advanced inference repo,\n",
            "[17:49.000 --> 17:53.000]  and I'm going to be focusing on the data extraction portion.\n",
            "[17:53.000 --> 17:56.000]  So, I'm going to go over to VS Code,\n",
            "[17:56.000 --> 17:58.000]  where I've got that open up,\n",
            "[17:58.000 --> 18:02.000]  and I'm going to go to the readme file.\n",
            "[18:02.000 --> 18:05.000]  So, in the readme, there's a set of instructions\n",
            "[18:05.000 --> 18:07.000]  for getting started.\n",
            "[18:07.000 --> 18:11.000]  The first step is to get an API started for OpenChat.\n",
            "[18:11.000 --> 18:15.000]  Now, I've provided a one-click template that you can use,\n",
            "[18:15.000 --> 18:18.000]  and this one-click template is freely accessible.\n",
            "[18:18.000 --> 18:21.000]  In fact, you can find a number of templates\n",
            "[18:21.000 --> 18:24.000]  at this public repo here, which is called one-click LLMs.\n",
            "[18:24.000 --> 18:30.000]  You can find one-click APIs for many models in TGI,\n",
            "[18:30.000 --> 18:33.000]  VLLM, and also a Lama CPP.\n",
            "[18:33.000 --> 18:37.000]  You can run them on RunPod, or you can run them on VLLM.\n",
            "[18:37.000 --> 18:39.000]  So, the ones that I'm going to get going\n",
            "[18:39.000 --> 18:44.000]  are the OpenChat 3.5 model, and then the Noteux 8x7B.\n",
            "[18:44.000 --> 18:48.000]  So, I've just clicked on that link to the OpenChat model,\n",
            "[18:48.000 --> 18:51.000]  and then typically I like to pick an A6000,\n",
            "[18:51.000 --> 18:53.000]  so I'll click here to deploy.\n",
            "[18:53.000 --> 18:56.000]  Everything should already be configured here.\n",
            "[18:56.000 --> 18:58.000]  Now, if you're going to run,\n",
            "[18:58.000 --> 19:02.000]  I recommend running it in full 16-bit precision.\n",
            "[19:02.000 --> 19:04.000]  You can run in AWQ.\n",
            "[19:04.000 --> 19:07.000]  It would allow you to run on a smaller amount of VRAM,\n",
            "[19:07.000 --> 19:11.000]  but this is a 7B model, so you only need about 15 gigabytes of VRAM.\n",
            "[19:11.000 --> 19:15.000]  So, an A4000 or an A6000 will fit it.\n",
            "[19:15.000 --> 19:18.000]  I recommend running in 16-bit format,\n",
            "[19:18.000 --> 19:22.000]  because that is actually one of the fastest formats\n",
            "[19:22.000 --> 19:25.000]  if you're going to concurrently ping the API,\n",
            "[19:25.000 --> 19:28.000]  which is what we want to do, because we want to process\n",
            "[19:28.000 --> 19:31.000]  many shards, many chunks of the text in one\n",
            "[19:31.000 --> 19:35.000]  to do things really quickly and get really good cost effectiveness.\n",
            "[19:35.000 --> 19:39.000]  Also, in 16-bit, we get better precision than 8-bit or 4-bit,\n",
            "[19:39.000 --> 19:42.000]  so that's going to be a benefit for performance too.\n",
            "[19:42.000 --> 19:44.000]  Now, one other tweak here is that\n",
            "[19:44.000 --> 19:48.000]  often I have Speculate 3 onto these one-click templates.\n",
            "[19:48.000 --> 19:52.000]  What this does is use Speculate of decoding based on the prompt,\n",
            "[19:52.000 --> 19:56.000]  so it will use the prompt to guide it in guessing some future tokens,\n",
            "[19:56.000 --> 20:00.000]  and this can provide a speed-up in decoding probably of about 30%.\n",
            "[20:00.000 --> 20:03.000]  However, if you're going to ping the API concurrently\n",
            "[20:03.000 --> 20:07.000]  with a lot of requests, it's actually better not to do speculation\n",
            "[20:07.000 --> 20:10.000]  because speculation does take up more of the GPU.\n",
            "[20:10.000 --> 20:13.000]  So, you can consider getting rid of the speculation.\n",
            "[20:13.000 --> 20:16.000]  It potentially will speed up performance a little bit\n",
            "[20:16.000 --> 20:18.000]  if you are going to ping the API concurrently,\n",
            "[20:18.000 --> 20:21.000]  which is relevant if you've got a very long piece of text,\n",
            "[20:21.000 --> 20:24.000]  maybe over 10,000 tokens.\n",
            "[20:24.000 --> 20:26.000]  And so, here are my pods.\n",
            "[20:26.000 --> 20:30.000]  I have Open Chat and I have Noteux 8x7B.\n",
            "[20:30.000 --> 20:35.000]  I am using AWQ for Noteux just so I can fit it on an A6000,\n",
            "[20:35.000 --> 20:39.000]  and you can see that it's already loaded and ready to go here.\n",
            "[20:39.000 --> 20:43.000]  And then I will have Open Chat, which should be pretty quick to download.\n",
            "[20:43.000 --> 20:46.000]  So, I'm copying the ID of the pod here,\n",
            "[20:46.000 --> 20:50.000]  and I'm going over to my VS Code,\n",
            "[20:50.000 --> 20:53.000]  and specifically, I want to go to the .env file.\n",
            "[20:53.000 --> 20:59.000]  And in my .env file, I'm going to just put in here my API endpoint,\n",
            "[21:00.000 --> 21:03.000]  so I've set up this endpoint here in RunPod.\n",
            "[21:03.000 --> 21:05.000]  It's going to hit the Open Chat,\n",
            "[21:05.000 --> 21:08.000]  and I've set my model to Open Chat, Open Chat 3.5.\n",
            "[21:08.000 --> 21:10.000]  Now, just a word of warning,\n",
            "[21:10.000 --> 21:13.000]  if you're using the blocks AWQ for 3.5,\n",
            "[21:13.000 --> 21:15.000]  there's currently no tokenizer template,\n",
            "[21:15.000 --> 21:17.000]  no tokenizer chat template,\n",
            "[21:17.000 --> 21:20.000]  so you should always use this as the base model\n",
            "[21:20.000 --> 21:23.000]  because setting the model here is what sets up the chat format.\n",
            "[21:23.000 --> 21:25.000]  So, use Open Chat here,\n",
            "[21:25.000 --> 21:27.000]  and if you're going to use Notux,\n",
            "[21:27.000 --> 21:29.000]  we'll set that one a little later in the video.\n",
            "[21:31.000 --> 21:33.000]  So, let's go back to the ReadMe,\n",
            "[21:33.000 --> 21:35.000]  and scroll up here.\n",
            "[21:36.000 --> 21:39.000]  And this is the ReadMe for the overall repo\n",
            "[21:39.000 --> 21:41.000]  for Advanced Inference,\n",
            "[21:41.000 --> 21:45.000]  but what I want is actually the ReadMe for the data extraction.\n",
            "[21:45.000 --> 21:47.000]  So, I'm going to close down that first ReadMe,\n",
            "[21:47.000 --> 21:49.000]  go back to the data extraction,\n",
            "[21:49.000 --> 21:51.000]  and I've got my pod launched.\n",
            "[21:51.000 --> 21:53.000]  I've got the .env file adjusted\n",
            "[21:53.000 --> 21:55.000]  so that I'm hitting that run pod.\n",
            "[21:55.000 --> 21:59.000]  By the way, if you're just using the run pod by yourself\n",
            "[21:59.000 --> 22:01.000]  without having access to the repo,\n",
            "[22:01.000 --> 22:03.000]  there are instructions in the ReadMe of the run pod\n",
            "[22:03.000 --> 22:06.000]  as to how you can make requests using the run pod ID.\n",
            "[22:07.000 --> 22:10.000]  Now, I'm going to CD into the data extraction folder.\n",
            "[22:10.000 --> 22:13.000]  So, let's open up the terminal here,\n",
            "[22:13.000 --> 22:18.000]  and let's CD into data extraction.\n",
            "[22:19.000 --> 22:21.000]  And next, I want to make sure\n",
            "[22:21.000 --> 22:23.000]  that I activate a virtual environment.\n",
            "[22:23.000 --> 22:26.000]  I've got this virtual environment called extractenv.\n",
            "[22:26.000 --> 22:28.000]  The very first time you run, you'll have to create it\n",
            "[22:28.000 --> 22:31.000]  with python-env extractenv,\n",
            "[22:31.000 --> 22:34.000]  and then you'll have to source that environment,\n",
            "[22:34.000 --> 22:36.000]  and then install the requirements.\n",
            "[22:36.000 --> 22:38.000]  I've already installed the requirements,\n",
            "[22:38.000 --> 22:40.000]  so I'm just going to source the environment,\n",
            "[22:40.000 --> 22:43.000]  and here we are in the extractenv now.\n",
            "[22:43.000 --> 22:47.000]  And with that, we're ready to immediately make calls to the API.\n",
            "[22:47.000 --> 22:49.000]  We can make JSON calls,\n",
            "[22:49.000 --> 22:52.000]  and we can also make YAML calls down here\n",
            "[22:52.000 --> 22:55.000]  just by providing a JSON or YAML flag.\n",
            "[22:55.000 --> 22:57.000]  So, we'll have an output format.\n",
            "[22:57.000 --> 23:01.000]  That's the JSON or YAML.\n",
            "[23:01.000 --> 23:04.000]  We have the chunk length measured in characters.\n",
            "[23:04.000 --> 23:06.000]  So, this is currently 8,000 characters,\n",
            "[23:06.000 --> 23:09.000]  which is about 2,000 tokens.\n",
            "[23:09.000 --> 23:11.000]  And I recommend this length\n",
            "[23:11.000 --> 23:14.000]  because it's about half of 4,000 tokens.\n",
            "[23:14.000 --> 23:18.000]  So, it's about half of the context length of a 4,000 token model.\n",
            "[23:18.000 --> 23:20.000]  And you like to have something fairly short\n",
            "[23:20.000 --> 23:23.000]  because the longer the context going into a language model,\n",
            "[23:23.000 --> 23:26.000]  the more difficult it will be to extract data from it.\n",
            "[23:26.000 --> 23:29.000]  We'll see even GPT-4 has got trouble\n",
            "[23:29.000 --> 23:32.000]  in very long contexts with extracting all of the data as required.\n",
            "[23:32.000 --> 23:35.000]  So, typically, I'm going to slice up my text\n",
            "[23:35.000 --> 23:37.000]  into chunks of 8,000 characters\n",
            "[23:37.000 --> 23:40.000]  and send those chunks through the GPU in parallel,\n",
            "[23:40.000 --> 23:43.000]  which the GPU is very efficient at doing.\n",
            "[23:43.000 --> 23:46.000]  And that's what this batching equals true parameter is.\n",
            "[23:46.000 --> 23:48.000]  It's just concurrent requests.\n",
            "[23:48.000 --> 23:52.000]  So, I'm not just going to send in one block of messages\n",
            "[23:52.000 --> 23:55.000]  for decoding, I'm going to send in blocks in parallel\n",
            "[23:55.000 --> 23:58.000]  that each have a copy of the prompt\n",
            "[23:58.000 --> 24:01.000]  but with a different chunk of text inside it.\n",
            "[24:01.000 --> 24:03.000]  And then, I'll get back the chunks\n",
            "[24:03.000 --> 24:06.000]  and assemble them together into a final list,\n",
            "[24:06.000 --> 24:08.000]  a final JSON object or YAML.\n",
            "[24:08.000 --> 24:11.000]  And the last parameter in here is the input file name.\n",
            "[24:11.000 --> 24:14.000]  So, I'm putting in one of three files,\n",
            "[24:14.000 --> 24:16.000]  which I'll just pull up.\n",
            "[24:16.000 --> 24:19.000]  I'll call them out because the font size is small on screen,\n",
            "[24:19.000 --> 24:22.000]  but the main file is Berkshire 23.\n",
            "[24:22.000 --> 24:25.000]  It's the transcript from the Berkshire Hathaway 2023 meeting.\n",
            "[24:25.000 --> 24:27.000]  It's about, I think, 60,000 tokens,\n",
            "[24:27.000 --> 24:30.000]  so about 240,000 characters in length.\n",
            "[24:30.000 --> 24:33.000]  Then, I've got a shortened one to 60,000 characters\n",
            "[24:33.000 --> 24:36.000]  and one shortened to 12,500 characters,\n",
            "[24:36.000 --> 24:39.000]  which is below 4,000 tokens in length.\n",
            "[24:40.000 --> 24:42.000]  So, I'm going to start off,\n",
            "[24:42.000 --> 24:44.000]  and we're just going to work with the shortest file\n",
            "[24:44.000 --> 24:46.000]  because I want to get some quick responses\n",
            "[24:46.000 --> 24:48.000]  just for demo purposes.\n",
            "[24:48.000 --> 24:50.000]  So, let's head back to the readme\n",
            "[24:50.000 --> 24:55.000]  and I'm going to run first off this TGI prompt here\n",
            "[24:55.000 --> 24:58.000]  using JSON.\n",
            "[24:58.000 --> 25:00.000]  Now, before I press Enter,\n",
            "[25:00.000 --> 25:02.000]  let's just check that the pod is running.\n",
            "[25:02.000 --> 25:05.000]  So, here we are with OpenChat.\n",
            "[25:05.000 --> 25:07.000]  It looks like the container is up\n",
            "[25:07.000 --> 25:09.000]  and it looks like when you see\n",
            "[25:09.000 --> 25:11.000]  invalid hosting down here,\n",
            "[25:11.000 --> 25:14.000]  that means that the API is ready to accept requests,\n",
            "[25:14.000 --> 25:18.000]  and that's why the GPU memory is up as it should be.\n",
            "[25:19.000 --> 25:22.000]  Okay, so we're going to press Enter,\n",
            "[25:22.000 --> 25:25.000]  and let's see if I did something wrong there.\n",
            "[25:25.000 --> 25:28.000]  I actually missed the first piece of pasting.\n",
            "[25:29.000 --> 25:32.000]  Actually, right here, we're processing the 60K,\n",
            "[25:32.000 --> 25:34.000]  and that's fine.\n",
            "[25:34.000 --> 25:36.000]  It'll probably work fine.\n",
            "[25:36.000 --> 25:40.000]  It just did, and I validate\n",
            "[25:40.000 --> 25:44.000]  after doing every chunk that the format it returns is valid.\n",
            "[25:44.000 --> 25:46.000]  So, here at the bottom, it's just telling me\n",
            "[25:46.000 --> 25:48.000]  that the error rate was 0%.\n",
            "[25:48.000 --> 25:50.000]  That doesn't mean it's gotten all the entries,\n",
            "[25:50.000 --> 25:53.000]  but it means that the errors in terms of validating\n",
            "[25:53.000 --> 25:55.000]  the JSON format were 0.\n",
            "[25:55.000 --> 25:57.000]  Now, actually, I've just modified this\n",
            "[25:57.000 --> 25:59.000]  because I want to do a quick example\n",
            "[25:59.000 --> 26:03.000]  that is short on the 12.5K file,\n",
            "[26:03.000 --> 26:05.000]  so just a shorter context.\n",
            "[26:06.000 --> 26:09.000]  And I'll just show you what's happening here in the terminal.\n",
            "[26:10.000 --> 26:13.000]  Because it's 12.5K tokens in length,\n",
            "[26:13.000 --> 26:15.000]  and I've specified the chunk length of 8,000,\n",
            "[26:15.000 --> 26:19.000]  there are two chunks, one of 8,000 and one of 4,500.\n",
            "[26:20.000 --> 26:25.000]  And here, you can see exactly what has been fed into the LLM,\n",
            "[26:25.000 --> 26:27.000]  so I'll show you the second chunk.\n",
            "[26:27.000 --> 26:31.000]  It's matching very much what we've put in to GPT-4.\n",
            "[26:31.000 --> 26:34.000]  So, as you can see, it's JSON,\n",
            "[26:34.000 --> 26:38.000]  and we have the prompt formatted for open chat,\n",
            "[26:38.000 --> 26:41.000]  then extract names and organizations from the provided text,\n",
            "[26:41.000 --> 26:43.000]  and return them in JSON format,\n",
            "[26:43.000 --> 26:46.000]  then we have the schema, then we have an example of the response,\n",
            "[26:46.000 --> 26:49.000]  then we've got some guidance, then we've got the text,\n",
            "[26:49.000 --> 26:51.000]  and then at the end of the text,\n",
            "[26:51.000 --> 26:54.000]  we've got now answer immediately and only in JSON format,\n",
            "[26:54.000 --> 26:58.000]  and I've printed out the screen here the response,\n",
            "[26:58.000 --> 27:02.000]  which is indeed a JSON of names and organizations,\n",
            "[27:02.000 --> 27:08.000]  and you can see that the tokens generated were about 139,\n",
            "[27:08.000 --> 27:10.000]  and about 26 tokens per second,\n",
            "[27:10.000 --> 27:15.000]  and the second JSON we got back was about...\n",
            "[27:16.000 --> 27:18.000]  Yeah, sorry, that was the second one, 26,\n",
            "[27:18.000 --> 27:21.000]  and the first one here was 22.\n",
            "[27:21.000 --> 27:24.000]  And you can already see here in this second one,\n",
            "[27:24.000 --> 27:25.000]  when we're using JSON,\n",
            "[27:25.000 --> 27:27.000]  that there is a little bit of repetition\n",
            "[27:27.000 --> 27:30.000]  within the list of organizations.\n",
            "[27:30.000 --> 27:33.000]  Now, there's one little tweak I want to show you on that point,\n",
            "[27:33.000 --> 27:36.000]  which is that in the chat completion script,\n",
            "[27:36.000 --> 27:40.000]  which is where we send the prompt into the language model,\n",
            "[27:40.000 --> 27:42.000]  there are parameters here,\n",
            "[27:42.000 --> 27:44.000]  including the maximum number of tokens to return,\n",
            "[27:44.000 --> 27:46.000]  do sample is false,\n",
            "[27:46.000 --> 27:48.000]  this is equivalent to setting temperature to zero,\n",
            "[27:48.000 --> 27:52.000]  because it means that the language model is not sampling across the distribution,\n",
            "[27:52.000 --> 27:55.000]  they're just picking the most likely response,\n",
            "[27:55.000 --> 27:58.000]  and there's one more here, which is called repetition penalty,\n",
            "[27:58.000 --> 28:01.000]  and this adds a penalty for repeating the same response,\n",
            "[28:01.000 --> 28:04.000]  so we can put this to 1.1.\n",
            "[28:04.000 --> 28:06.000]  By the way, if you put it to 1, it's not a penalty,\n",
            "[28:06.000 --> 28:08.000]  it's just multiplying everything by 1,\n",
            "[28:08.000 --> 28:12.000]  so 1.1 penalizes repetition a little bit,\n",
            "[28:12.000 --> 28:15.000]  and we can rerun the script and just take a look\n",
            "[28:15.000 --> 28:17.000]  at whether that helps us out.\n",
            "[28:18.000 --> 28:22.000]  So you can see indeed when we put in the repetition penalty,\n",
            "[28:22.000 --> 28:25.000]  it does avoid any repetition in the responses,\n",
            "[28:25.000 --> 28:27.000]  so that's a good thing,\n",
            "[28:27.000 --> 28:31.000]  but on the downside, you can see that the first JSON that came back\n",
            "[28:31.000 --> 28:35.000]  actually had not got any names or organizations included within it.\n",
            "[28:35.000 --> 28:39.000]  So I'll caution that it is possible to tweak performance a little bit\n",
            "[28:39.000 --> 28:41.000]  with repetition penalty,\n",
            "[28:41.000 --> 28:44.000]  I find it works a bit better using JSON when doing JSON\n",
            "[28:44.000 --> 28:46.000]  than when doing YAML.\n",
            "[28:46.000 --> 28:48.000]  If you're doing YAML, adding the repetition penalty,\n",
            "[28:48.000 --> 28:52.000]  often in my experience, leads to a worse performance.\n",
            "[28:52.000 --> 28:56.000]  So next, I'm going to just run the script for YAML\n",
            "[28:56.000 --> 28:59.000]  to show you that performance here.\n",
            "[28:59.000 --> 29:02.000]  So this is very similar script,\n",
            "[29:02.000 --> 29:04.000]  but this time with the YAML flag\n",
            "[29:04.000 --> 29:08.000]  and with the model responding in YAML format.\n",
            "[29:08.000 --> 29:13.000]  And here you can see, actually, I've run the one for a 60K of tokens,\n",
            "[29:13.000 --> 29:19.000]  so let's just run the same now with the shorter input text.\n",
            "[29:19.000 --> 29:23.000]  And here you can see it's now responding with YAML format,\n",
            "[29:23.000 --> 29:25.000]  so names and organizations,\n",
            "[29:25.000 --> 29:29.000]  and we're getting quite a nice response here for both.\n",
            "[29:29.000 --> 29:34.000]  So these outputs are now saved in output.yaml\n",
            "[29:34.000 --> 29:38.000]  for the YAML example, which we can bring up here.\n",
            "[29:38.000 --> 29:42.000]  And you can see the names that have been extracted in the orgs,\n",
            "[29:42.000 --> 29:47.000]  and then in output.json, you can see the names and the orgs.\n",
            "[29:47.000 --> 29:53.000]  And what I'm going to do, actually, is just rerun the example for JSON,\n",
            "[29:53.000 --> 29:57.000]  but I want to make sure that repetition penalty is off,\n",
            "[29:57.000 --> 29:59.000]  just for like for like comparison.\n",
            "[29:59.000 --> 30:02.000]  So I'll run JSON on the short text,\n",
            "[30:02.000 --> 30:05.000]  and that's going to update this JSON file.\n",
            "[30:05.000 --> 30:11.000]  Notice that when I do combine the JSON objects from different chunks,\n",
            "[30:11.000 --> 30:14.000]  you can do de-duplication, so you can remove duplicate answers,\n",
            "[30:14.000 --> 30:18.000]  and that definitely helps to clean up your response a little bit.\n",
            "[30:18.000 --> 30:24.000]  So here we have the response from the short text using the JSON method.\n",
            "[30:24.000 --> 30:32.000]  Now, the next thing I want to do is compare performance to running GPT-4 and GPT-3.5.\n",
            "[30:32.000 --> 30:35.000]  So we're going to do that and take a look.\n",
            "[30:35.000 --> 30:40.000]  And the best way to do that is to copy the exact prompt that we need.\n",
            "[30:40.000 --> 30:45.000]  So here I'm going to copy this JSON prompt.\n",
            "[30:45.000 --> 30:50.000]  And strictly speaking, I could do this in chunks as well,\n",
            "[30:50.000 --> 30:54.000]  but rather than do it in chunks, I'm just going to put it in as a single prompt.\n",
            "[30:54.000 --> 31:02.000]  So here we go back over, and we go to chat.ui, create a new chat,\n",
            "[31:02.000 --> 31:05.000]  and paste in this prompt here.\n",
            "[31:05.000 --> 31:10.000]  Now, I want to put in the full text. I don't want to put in just a chunk.\n",
            "[31:10.000 --> 31:16.000]  I know this isn't exactly apples, but it's just the quickest way I can show a demonstration.\n",
            "[31:16.000 --> 31:22.000]  So I'm going to copy all of this short text and paste it in here.\n",
            "[31:22.000 --> 31:28.000]  And now ask GPT-4Turbo for a response.\n",
            "[31:28.000 --> 31:33.000]  And you can see it's definitely a fairly slow tokens per second,\n",
            "[31:33.000 --> 31:38.000]  probably just a few tokens per second coming out from this model.\n",
            "[31:38.000 --> 31:43.000]  But it will allow us to compare performance pretty nicely.\n",
            "[31:43.000 --> 31:50.000]  All right, so that's GPT-4, and we're going to copy that and put it over into GPT-4\n",
            "[31:50.000 --> 31:56.000]  and remind me that's JSON, so I'll just copy it, paste in here.\n",
            "[31:56.000 --> 32:01.000]  And next, I'm going to do the same thing. I want to go back to this chat\n",
            "[32:01.000 --> 32:08.000]  and paste in the exact same thing.\n",
            "[32:08.000 --> 32:14.000]  And here we have the response from GPT-3.5, which is a little bit less neat,\n",
            "[32:14.000 --> 32:19.000]  but we'll paste it in here. So that's 3.5.\n",
            "[32:19.000 --> 32:21.000]  And next, we want to do the same for YAML.\n",
            "[32:21.000 --> 32:26.000]  So to do the YAML response, I can just rerun the script\n",
            "[32:26.000 --> 32:29.000]  just so I have it lower down here.\n",
            "[32:29.000 --> 32:33.000]  And here we go.\n",
            "[32:33.000 --> 32:39.000]  So all I want to do is copy the exact prompt again.\n",
            "[32:39.000 --> 32:42.000]  So answer immediately.\n",
            "[32:42.000 --> 32:47.000]  And I want to make sure that I get all of this in here.\n",
            "[32:47.000 --> 32:51.000]  So again, let's create a new chat. Let's do 3.5 first.\n",
            "[32:51.000 --> 32:57.000]  And again, because we want to just do everything at once, rather than chunks,\n",
            "[32:57.000 --> 33:04.000]  I'm just going to copy in the entire text here, paste here.\n",
            "[33:04.000 --> 33:08.000]  And GPT-3.5 should be quite a bit faster.\n",
            "[33:08.000 --> 33:11.000]  It's not exactly a format that I would like.\n",
            "[33:11.000 --> 33:16.000]  You can see it's actually not giving the correct format.\n",
            "[33:16.000 --> 33:20.000]  There is a little tweak that I know because I've been playing around.\n",
            "[33:20.000 --> 33:22.000]  That should allow us to get out YAML.\n",
            "[33:22.000 --> 33:27.000]  So here you can see already that GPT-3.5 Turbo is less robust.\n",
            "[33:27.000 --> 33:34.000]  But you can just say in code pen, let's see if that helps.\n",
            "[33:34.000 --> 33:36.000]  It does indeed.\n",
            "[33:36.000 --> 33:45.000]  So here we'll copy that and back over and into the YAML 3.5.\n",
            "[33:45.000 --> 33:47.000]  And save.\n",
            "[33:47.000 --> 33:51.000]  And then this whole prompt that we did for YAML for 3.5,\n",
            "[33:51.000 --> 33:53.000]  I'm just going to copy all of this.\n",
            "[33:53.000 --> 33:55.000]  Create a new chat.\n",
            "[33:55.000 --> 33:59.000]  This time for GPT-4, zero temperature.\n",
            "[33:59.000 --> 34:02.000]  And ask it, I've put in a code pen.\n",
            "[34:02.000 --> 34:09.000]  GPT-4 doesn't necessarily need that, but it's not going to do any harm.\n",
            "[34:09.000 --> 34:13.000]  So here we have the names and organizations.\n",
            "[34:13.000 --> 34:17.000]  And that's ready for me to copy.\n",
            "[34:17.000 --> 34:25.000]  And over here, I'm going to paste it into GPT-4 for the YAML.\n",
            "[34:25.000 --> 34:32.000]  Okay, so now we have the results in JSON and YAML for GPT-4, GPT-3.5.\n",
            "[34:32.000 --> 34:37.000]  And we also have the results of OpenChat 3.5 in JSON and YAML.\n",
            "[34:37.000 --> 34:41.000]  And there's a nice script that we can use in the readme file.\n",
            "[34:41.000 --> 34:43.000]  It's a comparison script.\n",
            "[34:43.000 --> 34:46.000]  And simply by running Python compare,\n",
            "[34:46.000 --> 34:53.000]  JSON, we can get a nice comparison of performance between all of these models.\n",
            "[34:53.000 --> 35:00.000]  So let me just expand my screen a little bit and see if we can get a better view.\n",
            "[35:00.000 --> 35:04.000]  So here's comparing the JSON performance.\n",
            "[35:04.000 --> 35:06.000]  First of all, on the names.\n",
            "[35:06.000 --> 35:12.000]  So we have our baseline model to OpenChat and then GPT-4 and then GPT-3.5.\n",
            "[35:12.000 --> 35:18.000]  And basically what you can see is that they're all pretty much on par.\n",
            "[35:18.000 --> 35:23.000]  It looks like GPT-3.5 is missing one or two answers.\n",
            "[35:23.000 --> 35:29.000]  So you can see, for example, Ron Olson is missing from GPT-3.5.\n",
            "[35:30.000 --> 35:35.000]  Whereas the output of OpenChat has made a little mistake here by including Geico.\n",
            "[35:35.000 --> 35:41.000]  Geico is not a person, Geico is a company, which I guess would be hard to know potentially,\n",
            "[35:41.000 --> 35:43.000]  but that's still a mistake.\n",
            "[35:43.000 --> 35:48.000]  Also, in a sense, there's a mistake here because OpenChat does not know that Ajit Jain\n",
            "[35:48.000 --> 35:54.000]  and Ajit Jain is, in fact, referring to the same person, which GPT-4 is aware of.\n",
            "[35:54.000 --> 35:58.000]  And you can also see that Munger is included in OpenChat,\n",
            "[35:58.000 --> 36:02.000]  whereas Charlie Munger is also included.\n",
            "[36:02.000 --> 36:08.000]  So you could say that this is an error, but actually within the transcript,\n",
            "[36:08.000 --> 36:11.000]  Munger is often referred to as just Munger.\n",
            "[36:11.000 --> 36:17.000]  But I guess with extra knowledge, then the model would be able to statistically recognize\n",
            "[36:17.000 --> 36:20.000]  that this refers to Charlie Munger.\n",
            "[36:20.000 --> 36:25.000]  So overall, I would say performance on the names here is relatively similar,\n",
            "[36:25.000 --> 36:31.000]  perhaps though a little bit higher of an error rate when you look at the output of OpenChat\n",
            "[36:31.000 --> 36:38.000]  and maybe a tiny benefit over GPT-3.5 just in terms of getting all of the entries.\n",
            "[36:38.000 --> 36:43.000]  Now, the next one here for organizations and here in JSON format,\n",
            "[36:43.000 --> 36:45.000]  we're definitely seeing some difference.\n",
            "[36:45.000 --> 36:46.000]  This is more challenging.\n",
            "[36:46.000 --> 36:49.000]  Of course, the second list is more challenging because they're both done at once\n",
            "[36:49.000 --> 36:55.000]  if you want better performance, probably only asking for one data type at once is a better idea.\n",
            "[36:55.000 --> 37:03.000]  But here you can see there's a fairly close match between the OpenChat and GPT-4.\n",
            "[37:03.000 --> 37:06.000]  GPT-4 includes Citi.\n",
            "[37:06.000 --> 37:07.000]  Citi is not a company.\n",
            "[37:07.000 --> 37:11.000]  Could be relating to Citibank because Jane Fraser is mentioned.\n",
            "[37:11.000 --> 37:14.000]  So that's somewhat incorrect, actually, should be CITI.\n",
            "[37:14.000 --> 37:23.000]  Meanwhile, Gaiko is not recognized by OpenChat and Occidental Petroleum is also missed by OpenChat in JSON format.\n",
            "[37:23.000 --> 37:27.000]  And in GPT-3.5, you can see that there are a few missing, too.\n",
            "[37:27.000 --> 37:32.000]  Occidental Petroleum is just written down as Oxy, which is the ticker.\n",
            "[37:32.000 --> 37:39.000]  And also it is missing, looks like it's missing BNSF and also missing CNBC.\n",
            "[37:39.000 --> 37:44.000]  So you can see here GPT-4 definitely performing better in this case.\n",
            "[37:44.000 --> 37:46.000]  Now let's look at the YAML results.\n",
            "[37:46.000 --> 37:55.000]  And before I press go on that, just take a note that GPT-4 is currently recognizing 15 names out of this initial text.\n",
            "[37:55.000 --> 37:59.000]  And it's recognizing 12 organizations.\n",
            "[37:59.000 --> 38:03.000]  So let's see what happens when we compare the YAML results.\n",
            "[38:03.000 --> 38:10.000]  And right away, you can see GPT-4 is predicting the same in YAML, at least in total number, although the answers are a little different.\n",
            "[38:10.000 --> 38:12.000]  We'll get to that.\n",
            "[38:12.000 --> 38:17.000]  And you can see in terms of organizations, we have 12 organizations.\n",
            "[38:17.000 --> 38:22.000]  So that's a little bit different as well in YAML than in JSON.\n",
            "[38:22.000 --> 38:25.000]  So you can see that even the most powerful model is not exactly consistent.\n",
            "[38:25.000 --> 38:28.000]  And it depends on whether you ask in JSON or in YAML.\n",
            "[38:28.000 --> 38:32.000]  Now, the performance of the OpenChat you can see is quite good here.\n",
            "[38:32.000 --> 38:41.000]  So OpenChat in YAML format is actually getting, it's largely getting, it's largely getting all of these.\n",
            "[38:41.000 --> 38:47.000]  You can even see it's saying mic in one case, which probably refers to mics in totally old, old or so many mics in the world.\n",
            "[38:47.000 --> 38:50.000]  I'm not sure you could really criticize that.\n",
            "[38:50.000 --> 38:54.000]  But really, the spelling here is a bit different.\n",
            "[38:54.000 --> 38:57.000]  They both think that Debbie has a different surname.\n",
            "[38:57.000 --> 39:03.000]  And you can also see that instead of having Warren Buffett, OpenChat just has Buffett.\n",
            "[39:03.000 --> 39:08.000]  But really quite good performance from OpenChat here on YAML.\n",
            "[39:08.000 --> 39:13.000]  Meanwhile, chat GPT-3.5 Turbo is definitely missing a few compared.\n",
            "[39:13.000 --> 39:19.000]  And interestingly, it doesn't capitalize sometimes the surnames, which I don't fully understand.\n",
            "[39:20.000 --> 39:28.000]  But here you can see with YAML, actually OpenChat is outperforming the GPT-3.5 model.\n",
            "[39:28.000 --> 39:31.000]  And it's doing really well compared to GPT-4.\n",
            "[39:31.000 --> 39:36.000]  And even when you look here at the organizations, there are a few things that are a bit questionable.\n",
            "[39:36.000 --> 39:40.000]  Like, is Omaha Convention Center an organization?\n",
            "[39:40.000 --> 39:46.000]  I mean, maybe it is, but it's probably not because it's a convention center that's owned by some company.\n",
            "[39:47.000 --> 39:55.000]  Other than that, you can see chat GPT-4 includes city again.\n",
            "[39:55.000 --> 40:04.000]  And it includes CNBC twice because it includes CNBC.com, which is really part of CNBC, so probably isn't a distinct entity.\n",
            "[40:04.000 --> 40:09.000]  Oxydental petroleum is also missed here in this example.\n",
            "[40:09.000 --> 40:15.000]  I will note that if you make some small tweaks, you sometimes can make it appear as accidental petroleum.\n",
            "[40:15.000 --> 40:22.000]  But broadly speaking, here it is, GPT-3.5 actually thinks it's accident petroleum.\n",
            "[40:22.000 --> 40:33.000]  So you can see in general OpenChat not too far from GPT-3.4 and in YAML format, we're getting a little bit better performance here.\n",
            "[40:33.000 --> 40:38.000]  Next up, I want to push the model by looking at a really long context length.\n",
            "[40:38.000 --> 40:46.000]  So for this, we're going to look at the entire Berkshire Hathaway meeting transcript, which I think is about 60,000 tokens in length.\n",
            "[40:46.000 --> 40:52.000]  And we're going to run this by YAML and we're going to run it on the OpenChat model.\n",
            "[40:52.000 --> 41:01.000]  So let's take a quick look here and go back to find a script that we can run.\n",
            "[41:01.000 --> 41:10.000]  So scrolling up, I'm going to grab the YAML script, but I'm going to adjust it so that we're looking at the full file.\n",
            "[41:10.000 --> 41:13.000]  And we are indeed going to run it in YAML.\n",
            "[41:13.000 --> 41:19.000]  So let's just go ahead and paste this here.\n",
            "[41:19.000 --> 41:35.000]  And what's happening now is that within the script called TGI data extraction, there is a concurrent mechanism set up so that there are multiple threads that are being executed in parallel.\n",
            "[41:35.000 --> 41:48.000]  So we're sending all of these messages into the GPU and text generation inference, the API, is batching them all together so that we're operating in parallel across the GPU with all of these requests.\n",
            "[41:48.000 --> 41:52.000]  And that allows us to get really fast inference.\n",
            "[41:52.000 --> 41:59.000]  As you can see here, we've already processed about 45 requests, probably in 30 seconds to be honest.\n",
            "[41:59.000 --> 42:03.000]  And we have an error rate of about 6% to 7%.\n",
            "[42:03.000 --> 42:08.000]  So you can see here that in one case, there was a repetition problem.\n",
            "[42:08.000 --> 42:14.000]  So we got a lot of Coca-Cola from this invalid YAML.\n",
            "[42:14.000 --> 42:20.000]  But we had a lot of the other files, more than 90%, that were OK.\n",
            "[42:20.000 --> 42:29.000]  And so if you do start to ping with a very large number of examples, it is difficult to avoid all errors.\n",
            "[42:29.000 --> 42:33.000]  But this still will compile into an output file.\n",
            "[42:33.000 --> 42:40.000]  So we have output.yaml here, and we've extracted much of the text.\n",
            "[42:40.000 --> 42:46.000]  So here we have all of the names, and there are quite a few.\n",
            "[42:46.000 --> 42:50.000]  And here we have all of the organizations.\n",
            "[42:50.000 --> 42:56.000]  And you can see that they're listed alphabetically, because that's how the extractor combines them.\n",
            "[42:56.000 --> 43:01.000]  And so yeah, this is a massive target here, a massive project.\n",
            "[43:01.000 --> 43:09.000]  So let's just take a look and put that whole context into GPT for Turbo, an expensive endeavor.\n",
            "[43:09.000 --> 43:11.000]  But let's try it.\n",
            "[43:11.000 --> 43:15.000]  So I'm going to go back to this example here, where I had the YAML.\n",
            "[43:15.000 --> 43:20.000]  And what I'm going to do is just swap out the text.\n",
            "[43:20.000 --> 43:28.000]  So that instead of putting in the short piece of text, I'll put in the full context\n",
            "[43:28.000 --> 43:36.000]  and let the long context of GPT for Turbo handle that.\n",
            "[43:36.000 --> 43:39.000]  We'll talk a little bit about cost towards the end of this video.\n",
            "[43:39.000 --> 43:45.000]  You can see already that it's definitely slow if you're going to put in this length of context.\n",
            "[43:45.000 --> 43:51.000]  But let's just take a look at how this comes out in terms of answers.\n",
            "[43:51.000 --> 44:04.000]  And what I can tell you is that even with a very powerful model like GPT for Turbo, it's going to leave out certain answers.\n",
            "[44:04.000 --> 44:08.000]  Although it does a good job of getting quite a lot.\n",
            "[44:08.000 --> 44:12.000]  And there are very many organizations within this example here.\n",
            "[44:12.000 --> 44:16.000]  And this indeed is a very long answer here.\n",
            "[44:16.000 --> 44:23.000]  But what I want to just show you as a key point is that GPT for is not able to get all of the entries.\n",
            "[44:23.000 --> 44:35.000]  For example, if you look here at the output that we got from open chat, if I just pick an example here that open chat is finding and check here within the input text,\n",
            "[44:35.000 --> 44:40.000]  you can see that Wally Weiss indeed does appear within the input text.\n",
            "[44:40.000 --> 44:51.000]  This I think might be a listing of the directors, but it does not appear or rather Wally does not appear within the list that GPT for outputs.\n",
            "[44:51.000 --> 44:54.000]  So there's no appearance of Wally Weiss here.\n",
            "[44:54.000 --> 44:58.000]  And the same is true for other characters too, like Merle Whitburn.\n",
            "[44:58.000 --> 45:08.000]  If you look for Merle Whitburn here, that name does appear within the output of open chat, but does not appear within the output of GPT for.\n",
            "[45:08.000 --> 45:15.000]  So definitely just using a really long context, even with a very powerful model is not going to get you strong accuracy.\n",
            "[45:15.000 --> 45:26.000]  Although if you chunk that up and send in chunks to GPT for no doubt, you're going to get better performance much as we are here because we're doing parallel chunking with smaller chunks.\n",
            "[45:26.000 --> 45:31.000]  And as you can see, when you do smaller chunks, you do get more repetition.\n",
            "[45:31.000 --> 45:34.000]  I've de-duplicated and removed any repeating.\n",
            "[45:34.000 --> 45:40.000]  But even still, I'm going to get repetition in the sense that Howard and Howard Buffett are going to appear as different characters.\n",
            "[45:40.000 --> 45:50.000]  And so you may want to run through an add-alarm again to kind of reduce this down if you see that there are names that are quite similar, like these two names here.\n",
            "[45:50.000 --> 45:54.000]  And you want to filter those out further.\n",
            "[45:54.000 --> 45:58.000]  Now, I had said I'd take a quick look at the Nautox model.\n",
            "[45:58.000 --> 46:02.000]  So let's do that. I'll just close off some of my files here.\n",
            "[46:02.000 --> 46:06.000]  And I'm going to need to adjust my .env file.\n",
            "[46:06.000 --> 46:08.000]  So let's open that up here.\n",
            "[46:08.000 --> 46:16.000]  I'll go back over to run pod and grab the pod ID from Nautox, which is up and running.\n",
            "[46:16.000 --> 46:21.000]  So with that, I will paste in here that API endpoint.\n",
            "[46:21.000 --> 46:29.000]  And I do need to get the name of the repo so I can grab the chat template.\n",
            "[46:29.000 --> 46:34.000]  So what I'll do here is I'll do model equals and paste in block.\n",
            "[46:34.000 --> 46:37.000]  So this is the AWQ model from the block.\n",
            "[46:37.000 --> 46:39.000]  I always had to up to the block.\n",
            "[46:39.000 --> 46:43.000]  And we're ready now to do a quick run.\n",
            "[46:43.000 --> 46:45.000]  I won't run the really long text.\n",
            "[46:45.000 --> 46:51.000]  I think maybe I can try and run the 6DK text like this using Nautox.\n",
            "[46:51.000 --> 46:59.000]  Let's run it first in YAML format and see what Nautox comes up with.\n",
            "[46:59.000 --> 47:04.000]  So you can see the prompt format for Nautox is like mixed trial, which is like Lama.\n",
            "[47:04.000 --> 47:07.000]  It's this inst type format.\n",
            "[47:07.000 --> 47:09.000]  And we'll see how it performs.\n",
            "[47:09.000 --> 47:12.000]  It's much slower because it's a larger model.\n",
            "[47:12.000 --> 47:23.000]  It's about 56 billion parameters instead of 7, so about 8 times larger than the 7B.\n",
            "[47:23.000 --> 47:33.000]  And when that's done, we'll run a quick example using JSON as well.\n",
            "[47:33.000 --> 47:38.000]  So we're just getting back to the start of the responses, the batch of responses now from Nautox.\n",
            "[47:38.000 --> 47:43.000]  And I can see already just from the errors, the error rate is about a half.\n",
            "[47:43.000 --> 47:49.000]  So the syntax of the AML file was broken in about half of these eight cases.\n",
            "[47:49.000 --> 47:56.000]  And if you want to scroll up, you can see some of the issues here.\n",
            "[47:56.000 --> 47:58.000]  That one there it looks.\n",
            "[47:58.000 --> 48:04.000]  Yeah, so basically names wasn't provided in one of these objects.\n",
            "[48:05.000 --> 48:08.000]  So actually instead of responding with names and then a list of names,\n",
            "[48:08.000 --> 48:11.000]  it's actually responding, including properties and items.\n",
            "[48:11.000 --> 48:16.000]  So it's getting confused by the schema that I provided.\n",
            "[48:16.000 --> 48:21.000]  You might argue that not including the schema might have helped somehow in this case.\n",
            "[48:21.000 --> 48:27.000]  Without too much further ado, let's just run a quick example on JSON\n",
            "[48:27.000 --> 48:28.000]  and see how that performs.\n",
            "[48:28.000 --> 48:31.000]  I'm running it on the shorter piece of text.\n",
            "[48:31.000 --> 48:35.000]  So there'll just be two requests being sent in in parallel.\n",
            "[48:35.000 --> 48:39.000]  And again, you can consider increasing the repetition penalty a little bit for JSON\n",
            "[48:39.000 --> 48:44.000]  to try and reduce repetition in the JSON responses.\n",
            "[48:44.000 --> 48:50.000]  And here indeed, we get out a reasonable set of answers.\n",
            "[48:50.000 --> 48:54.000]  So that's not too bad at all.\n",
            "[48:54.000 --> 48:58.000]  The list itself looks a little bit shorter than what we got with OpenChat,\n",
            "[48:58.000 --> 49:01.000]  so I'm not sure it's exactly including everything.\n",
            "[49:01.000 --> 49:06.000]  Likewise here, we've got eight entries, whereas there's actually about 12.\n",
            "[49:06.000 --> 49:10.000]  So we're getting some sensible responses here from Noteux.\n",
            "[49:10.000 --> 49:13.000]  Now, sorry, these are not the consolidated ones.\n",
            "[49:13.000 --> 49:18.000]  So what I should do is compare the consolidated answers.\n",
            "[49:18.000 --> 49:25.000]  Maybe I can do that if I just run a quick comparison of the JSON.\n",
            "[49:25.000 --> 49:35.000]  So what I can do is run compare, sorry, Python, compare.pyJSON.\n",
            "[49:35.000 --> 49:38.000]  I think that will do it.\n",
            "[49:38.000 --> 49:41.000]  Make a little more space.\n",
            "[49:41.000 --> 49:42.000]  And here we go.\n",
            "[49:42.000 --> 49:47.000]  So this is again comparing with the results of GPT-4 and GPT-3.5.\n",
            "[49:47.000 --> 49:50.000]  It has a different spelling for a deep chain.\n",
            "[49:50.000 --> 49:55.000]  But indeed, when we consolidated, and I forgot there were two different parts that need to be consolidated,\n",
            "[49:55.000 --> 50:01.000]  the performance here on the Noteux model is not bad.\n",
            "[50:01.000 --> 50:11.000]  Just looking at it visually, you can see there's that extra repetition of a cheat chain.\n",
            "[50:11.000 --> 50:18.000]  And there's also Buffet being added in as an addition to Warren Buffet.\n",
            "[50:18.000 --> 50:22.000]  So that's a second entry that's there twice, and there's also Mike there on its own.\n",
            "[50:22.000 --> 50:27.000]  So basically it's matching GPT-4 with the addition of some single names.\n",
            "[50:27.000 --> 50:34.000]  And when we look at organizations, you can see that it's doing quite well.\n",
            "[50:34.000 --> 50:38.000]  It's actually matching most of these.\n",
            "[50:38.000 --> 50:46.000]  It's not getting Occidental Petroleum, and it's adding in Nebraska, which is a city, not an organization.\n",
            "[50:46.000 --> 50:52.000]  And it also must be missing one, because I see that it doesn't have Occidental.\n",
            "[50:52.000 --> 50:57.000]  Yeah, it's got the convention center here, like OpenChat had previously.\n",
            "[50:57.000 --> 51:01.000]  However, it's performing quite a bit better than GPT-3.5.\n",
            "[51:01.000 --> 51:09.000]  So you can see that in JSON format, the Noteux model is performing reasonably well.\n",
            "[51:09.000 --> 51:13.000]  Although the performance was not quite as good when we were in YAML.\n",
            "[51:13.000 --> 51:16.000]  You can just double check that I have no repetition penalty.\n",
            "[51:16.000 --> 51:21.000]  So it just shows you you don't necessarily need to have repetition penalty for JSON performance to run well.\n",
            "[51:21.000 --> 51:28.000]  But because Noteux is quite a bit bigger as a model, and because the token generation rate is going to be quite a bit lower,\n",
            "[51:28.000 --> 51:33.000]  we can maybe just scroll up and see how fast those tokens were generated.\n",
            "[51:33.000 --> 51:35.000]  So here we go.\n",
            "[51:35.000 --> 51:38.000]  About six tokens per second instead of 22.\n",
            "[51:38.000 --> 51:45.000]  So it will take you probably about at least four times the time if you're going to be running using this larger model.\n",
            "[51:45.000 --> 51:50.000]  And because of that, there's definitely a cost advantage if you go and run with the OpenChat model,\n",
            "[51:50.000 --> 51:54.000]  which is what we're going to get to right now with respect to costs.\n",
            "[51:54.000 --> 51:57.000]  To pull this all together, I want to talk about the costs of different approaches.\n",
            "[51:57.000 --> 52:03.000]  So starting with GPT-4, the price currently is about one cent for every thousand tokens.\n",
            "[52:03.000 --> 52:07.000]  Now, you do pay additionally for output tokens, but you'll typically have a lot less output tokens,\n",
            "[52:07.000 --> 52:10.000]  so most of the cost is associated with the input.\n",
            "[52:10.000 --> 52:13.000]  GPT-3.5, meanwhile, is about a tenth of that cost.\n",
            "[52:13.000 --> 52:19.000]  So as an example, if you are trying to extract text from 100k context tokens,\n",
            "[52:19.000 --> 52:24.000]  that's going to cost about a dollar if you're going to do GPT-3.4.\n",
            "[52:24.000 --> 52:26.000]  Sorry, GPT-4.\n",
            "[52:26.000 --> 52:31.000]  Now, if you're using GPT-3.5, it's going to cost about ten cents.\n",
            "[52:31.000 --> 52:36.000]  Meanwhile, if you run an A6000 as I did on RunPod, and that's 60 cents,\n",
            "[52:36.000 --> 52:40.000]  now on Vast.ai, sometimes you can run them for 30 cents, or you could run an A4000,\n",
            "[52:40.000 --> 52:46.000]  and probably run one for as low as maybe even 10 or 15 cents on Vast.ai.\n",
            "[52:46.000 --> 52:50.000]  I will note, though, that the user experience is better on RunPod,\n",
            "[52:50.000 --> 52:53.000]  so that's why I end up using that more often.\n",
            "[52:53.000 --> 52:58.000]  Either way, you're going to have some less than a dollar per hour cost for running a server.\n",
            "[52:58.000 --> 53:04.000]  And as you saw in my demo, for running about 50,000 tokens, it's about one minute.\n",
            "[53:04.000 --> 53:07.000]  Maybe a bit less than that, about 30 seconds in what I showed.\n",
            "[53:07.000 --> 53:12.000]  So that makes it somewhere around half a cent to one cent for 50,000 tokens,\n",
            "[53:12.000 --> 53:19.000]  and so for doing 100k tokens, which I've got written incorrectly here,\n",
            "[53:19.000 --> 53:24.000]  for 100k tokens, it's going to be just two cents.\n",
            "[53:24.000 --> 53:26.000]  And when we bring that all together as a comparison,\n",
            "[53:26.000 --> 53:30.000]  GPT-4, it's going to cost you to extract from a 100k context.\n",
            "[53:30.000 --> 53:32.000]  It's going to cost you about a dollar.\n",
            "[53:32.000 --> 53:35.000]  GPT-3.5, it's going to cost you 10 cents.\n",
            "[53:35.000 --> 53:38.000]  And with an A6000, it's going to cost you about two cents,\n",
            "[53:38.000 --> 53:42.000]  although probably you could get it down half a cent if you use a cheaper GPU.\n",
            "[53:42.000 --> 53:46.000]  And also, it was actually a bit quicker than my estimate here.\n",
            "[53:46.000 --> 53:50.000]  Now, this is kind of unique because it's quite rare to be able to beat the cost\n",
            "[53:50.000 --> 53:54.000]  of just using the OpenAI API.\n",
            "[53:54.000 --> 53:58.000]  And the reason is that OpenAI have got the massive advantage\n",
            "[53:58.000 --> 54:01.000]  that they have got so many requests from customers,\n",
            "[54:01.000 --> 54:04.000]  they can send in very large batches to their GPUs.\n",
            "[54:04.000 --> 54:07.000]  And GPUs are very effective at processing batches\n",
            "[54:07.000 --> 54:12.000]  because they're constrained by the rate at which you read the model weights into the GPU.\n",
            "[54:12.000 --> 54:14.000]  And when you read in the model weights,\n",
            "[54:14.000 --> 54:19.000]  it's pretty much almost free to be able to do the computation many times.\n",
            "[54:19.000 --> 54:22.000]  You don't get bottlenecked by computation\n",
            "[54:22.000 --> 54:27.000]  until you're up at larger batch sizes like 16 or 32 or even much larger.\n",
            "[54:27.000 --> 54:31.000]  And so it's very favorable if you can have large batch sizes,\n",
            "[54:31.000 --> 54:34.000]  which most companies and individuals will not have\n",
            "[54:34.000 --> 54:37.000]  because they're only doing single requests.\n",
            "[54:37.000 --> 54:42.000]  However, if you hit your API concurrently, as I showed in this tutorial,\n",
            "[54:42.000 --> 54:45.000]  if you do a concurrent request with many, many chunks,\n",
            "[54:45.000 --> 54:49.000]  you're able to fill up your GPU and get very good utilization.\n",
            "[54:49.000 --> 54:52.000]  It's only because of that that I think you can get down\n",
            "[54:52.000 --> 54:56.000]  and competitive with the cost of using more expensive services.\n",
            "[54:56.000 --> 55:01.000]  Of course, the quality, as we showed, is probably not quite as good as GPT-4.\n",
            "[55:01.000 --> 55:05.000]  The best performance for data extraction will be using short contexts,\n",
            "[55:05.000 --> 55:11.000]  maybe about 2,000 tokens on GPT-4 and sending chunks in parallel.\n",
            "[55:11.000 --> 55:18.000]  But really, you can get quite close using the 7B model of OpenChat 3.5.\n",
            "[55:18.000 --> 55:23.000]  And that's it for data extraction in JSON and YAML format.\n",
            "[55:23.000 --> 55:28.000]  If you have any questions on how this works, just give me some comments down below.\n",
            "[55:28.000 --> 55:30.000]  If you want to get access just to these scripts,\n",
            "[55:30.000 --> 55:34.000]  you can find the link under advanced inference on trellis.com.\n",
            "[55:34.000 --> 55:38.000]  You can also there buy access, lifetime access,\n",
            "[55:38.000 --> 55:41.000]  to the GitHub repo for advanced inference.\n",
            "[55:41.000 --> 55:43.000]  All right, folks, all the best.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grab an audio file you have uploaded to Colab\n",
        "Click the folder icon in the left hand toolbar, and upload your audio file in m4a or mp3."
      ],
      "metadata": {
        "id": "hnZOa4A4O1rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"/content/llm-lingo.m4a\" --model small --language English"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYtbcHCPPD6j",
        "outputId": "a7d3e148-f98a-4cc0-be13-c4d0569def50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:09<00:00, 49.0MiB/s]\n",
            "[00:00.000 --> 00:05.000]  PHY2 gets an MIT license.\n",
            "[00:05.000 --> 00:08.000]  Lightning Attention 2.\n",
            "[00:08.000 --> 00:14.000]  It's the first linear implementation for attention computation.\n",
            "[00:14.000 --> 00:19.000]  A technical report on mixed trial 8x7B.\n",
            "[00:19.000 --> 00:28.000]  KPO, DPO and IPO.\n",
            "[00:28.000 --> 00:32.000]  Direct preference optimization.\n",
            "[00:32.000 --> 00:36.000]  Solar 10.7B.\n",
            "[00:36.000 --> 00:43.000]  This involves adding layers from a mixed trial 7B to a mixed trial 7B model.\n",
            "[00:43.000 --> 00:45.000]  OpenChat.\n",
            "[00:45.000 --> 00:53.000]  One of the very best LLMs that does not use preference optimization.\n",
            "[00:53.000 --> 00:58.000]  Noteux 8x7B V1.\n",
            "[00:58.000 --> 01:04.000]  This is a model that uses DPO on top of more DPO.\n",
            "[01:04.000 --> 01:11.000]  Gemini Pro versus GPT 3.5.\n",
            "[01:11.000 --> 01:14.000]  Microsoft PHY2.\n",
            "[01:14.000 --> 01:18.000]  A better and stronger model than PHY1.5.\n",
            "[01:18.000 --> 01:20.000]  Desi LM7B.\n",
            "[01:20.000 --> 01:24.000]  A very fast 7 billion parameter model.\n",
            "[01:24.000 --> 01:27.000]  Spin SPIN.\n",
            "[01:27.000 --> 01:31.000]  Self play fine tuning that improves LLMs.\n",
            "[01:31.000 --> 01:33.000]  Trixie.\n",
            "[01:33.000 --> 01:38.000]  Which is fast inference involving sparsity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running one of the above cells, you'll find the transcript (a vtt file) in your folder."
      ],
      "metadata": {
        "id": "XBzQvdWbWch2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "biPhaxG7LHPj",
        "qOtm7PV0WwR3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}